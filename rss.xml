<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Blake Chan]]></title><description><![CDATA[Blake Chan (Blakechan) be the change you wish to see in the world.]]></description><link>https://ai-chen2050.github.io/</link><generator>RSS for Node</generator><lastBuildDate>Wed, 27 Dec 2023 09:52:40 GMT</lastBuildDate><atom:link href="https://ai-chen2050.github.io//rss.xml" rel="self" type="application/rss+xml"/><copyright><![CDATA[Blakechan 2018-2022]]></copyright><language><![CDATA[en]]></language><item><title><![CDATA[Attention / Conv 大锅烩]]></title><description><![CDATA[<p>长期记录和实现看过的各种论文里的自注意力和卷积机制</p>
]]></description><link>https://ai-chen2050.github.io//post/2021/08/31/attention-conv/</link><guid isPermaLink="true">https://ai-chen2050.github.io//post/2021/08/31/attention-conv/</guid><dc:creator><![CDATA[Renovamen]]></dc:creator><pubDate>Tue, 31 Aug 2021 00:00:00 GMT</pubDate></item><item><title><![CDATA[自然梯度下降]]></title><description><![CDATA[<p>自然梯度下降（Natural Gradient Decent）把参数看成一种概率分布，然后使用 KL 散度而不是欧氏距离来作为距离的度量，从而更好地描述更新后的分布和原分布有多大的不同。</p>
]]></description><link>https://ai-chen2050.github.io//post/2021/07/28/natural-gradient/</link><guid isPermaLink="true">https://ai-chen2050.github.io//post/2021/07/28/natural-gradient/</guid><dc:creator><![CDATA[Renovamen]]></dc:creator><pubDate>Wed, 28 Jul 2021 00:00:00 GMT</pubDate></item><item><title><![CDATA[Fisher 信息矩阵]]></title><description><![CDATA[<p>Fisher 信息矩阵的数学意义和直观上的理解。</p>
]]></description><link>https://ai-chen2050.github.io//post/2021/07/27/fisher-information-matrix/</link><guid isPermaLink="true">https://ai-chen2050.github.io//post/2021/07/27/fisher-information-matrix/</guid><dc:creator><![CDATA[Renovamen]]></dc:creator><pubDate>Tue, 27 Jul 2021 00:00:00 GMT</pubDate></item></channel></rss>